from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain_groq import ChatGroq
from google.generativeai import configure as genai_configure
import os
import requests
import re
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from bs4 import BeautifulSoup
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph
from reportlab.lib.styles import getSampleStyleSheet
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from PyPDF2 import PdfReader
from dotenv import load_dotenv
from datetime import datetime

current_date = datetime.now().date()

load_dotenv()

# Configure Google Generative AI
genai_configure(api_key=os.getenv("GEMINI_API_KEY"))

# Initialize LLM models
jarvis_llm = ChatGroq(model='llama-3.1-70b-versatile')
supreme_llm = ChatGroq(model='llama-3.1-70b-versatile')
google_search_llm = ChatGroq(model='llama-3.1-70b-versatile')

# Initialize the memory buffer for J.A.R.V.I.S.
memory = ConversationBufferMemory()

# Initialize the ConversationChain with memory and the Groq LLM
conversation_chain = ConversationChain(
    llm=jarvis_llm,
    memory=memory
)

# Function to handle user input with J.A.R.V.I.S.
def handle_user_input(user_input):
    prompt = f'''
    You are a powerful LLM named J.A.R.V.I.S. you have to respond in the tone similar to Jarvis from Iron man but dont refer to anything from that movie i just want your tone to be like jarvis and not you actually talking about Tony stark stuffs and all. If you don't know the answer to the query or you think you might need to get some more additional knowledge from the internet or if you need any real-time information about anything or asked to check any real-time information about anything or you didn't get the required information from the Supreme Response, please respond with "GOOOOOGLEIIIIT"  to use the supreme llm powers to respond a particular question where you might need realtime browsing capabilities and specify what should be searched on Google to get the relevant information. If the query contains "Supreme Response" anywhere in the start then understand this that this is not a query but the response generated by a supreme model, then you should query the user for a query that you were not able to answer and understand the response from the supreme model and print that same response in a more formatted way. Also note that Supreme model is able to generate response from live internet browsing so you can use it when you think you might need its help.
    
    Query:
    {user_input}

    Answer:
    '''
    # Generate response using ConversationChain
    response = conversation_chain(prompt)
    return response["response"]

# Function to invoke Supreme Model with the query
def invoke_supreme_llm(query):
     
    prompt =f"Generate only one most effective Google search query to find information about the query provided to you and this is the current date {current_date} you can use this in your effective search query whenever you think you might need to use the date for generating the most effective Google search query and don't say anything else other then the the most effective Google search query"
    
    messages = [
        ("system", f"Generate only one most effective Google search query to find information about the query provided to you and this is the current date {current_date} you can use this in your effective search query whenever you think you might need to use the date for generating the most effective Google search query and don't say anything else other then the the most effective Google search query"),
        ("human", query),
    ]
    google_result = google_search_llm.invoke(messages)
    google_response = google_result.content.strip()
    return google_response

# Function to perform Google search and get URLs
def google_search(query, api_key, cx):
    url = f"https://www.googleapis.com/customsearch/v1?q={query}&num=5&key={api_key}&cx={cx}"
    try:
        response = requests.get(url)
        response.raise_for_status()
        search_results = response.json()
        all_urls = [item['link'] for item in search_results.get('items', [])]
        urls = [url for url in all_urls if 'youtube.com' not in url and 'shiksha.com' not in url and 'tiktok.com' not in url and 'youtu.be' not in url]
        while len(urls) < 5:
            remaining = 5 - len(urls)
            next_url = f"https://www.googleapis.com/customsearch/v1?q={query}&num={remaining}&start={len(all_urls)+1}&key={api_key}&cx={cx}"
            response = requests.get(next_url)
            response.raise_for_status()
            search_results = response.json()
            additional_urls = [item['link'] for item in search_results.get('items', [])]
            urls.extend([url for url in additional_urls if 'youtube.com' not in url and 'youtu.be' not in url])
            all_urls.extend(additional_urls)
            if not additional_urls or len(urls) >= 5:
                break
        return urls[:5]
    except requests.exceptions.HTTPError as err:
        return f"HTTP error occurred: {err}"
    except Exception as e:
        return f"Error occurred: {e}"

# Function to scrape HTML content from a URL
def scrape_html(url):
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        text_content = soup.get_text()
        text_content = re.sub(r'\n\s*\n', '\n', text_content)
        text_content = re.sub(r'\s+', ' ', text_content)
        return text_content.strip()
    except requests.exceptions.HTTPError as err:
        return f"HTTP error occurred: {err}"
    except Exception as e:
        return f"Error occurred: {e}"

# Ensure 'scraped' directory exists
scraped_dir = 'scraped'
if not os.path.exists(scraped_dir):
    os.makedirs(scraped_dir)

# Function to save text content to a .txt file inside the 'scraped' folder
def save_text_as_txt(text_content, filename):
    try:
        file_path = os.path.join(scraped_dir, filename)
        with open(file_path, 'w', encoding='utf-8') as file:
            file.write(text_content)
    except Exception as e:
        return f"Error occurred while saving to {file_path}: {e}"

# Function to merge multiple text files into a single PDF
def merge_text_files_to_pdf(text_files, output_pdf):
    try:
        doc = SimpleDocTemplate(output_pdf, pagesize=letter)
        elements = []
        styles = getSampleStyleSheet()
        normal_style = styles['Normal']
        for text_file in text_files:
            if os.path.exists(text_file):
                with open(text_file, 'r', encoding='utf-8') as file:
                    content = file.read().strip()
                if content:
                    paragraph = Paragraph(content, normal_style)
                    elements.append(paragraph)
        if elements:
            doc.build(elements)
        else:
            return f"No valid content to merge into {output_pdf}."
    except Exception as e:
        return f"Error occurred: {e}"

# Function to extract text from uploaded PDF files

def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        try:
            pdf_reader = PdfReader(pdf)
            for page in pdf_reader.pages:
                text += page.extract_text()
        except Exception as e:
            continue
    return text

# Function to split text into chunks
def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=10000, chunk_overlap=1000)
    chunks = text_splitter.split_text(text)
    return chunks

# Function to create and save vector store using FAISS
def create_vector_store(text_chunks):
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=os.getenv("GEMINI_API_KEY"))
    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
    vector_store.save_local("faiss_index")

# Function to perform question answering using the supreme model (llm)
def get_supreme_model_response(user_question):
    # Read combined PDF content and split into chunks
    combined_text = get_pdf_text([open(pdf, 'rb') for pdf in ["combined_content.pdf"]])
    text_chunks = get_text_chunks(combined_text)
    create_vector_store(text_chunks)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=os.getenv("GEMINI_API_KEY"))
    new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    llm = ChatGroq(model_name="llama-3.1-70b-versatile",temperature=0.35)

    prompt_template = """
        Answer the question as detailed as possible from the provided context, make sure to provide all the details you can use your brain to answer the question based on the given context and if you don't get the exact answer that you are looking for try to provide a valuable response yourself, by understanding the question and finding any slightly relevant insights from the content and Generate your own response and please be completely descriptive as if your response would be used for generating response by other llm
        \n\n
        Context:\n {context}?\n
        Question: \n{question}\n
        Answer:
    """
    chain = load_qa_chain(llm, chain_type="stuff", prompt=PromptTemplate(template=prompt_template, input_variables=["context", "question"]))
    context_value = "your_context_value"
    docs = new_db.similarity_search(user_question)
    response = chain({"input_documents": docs, "context": context_value, "question": user_question}, return_only_outputs=True)
    return response["output_text"]


